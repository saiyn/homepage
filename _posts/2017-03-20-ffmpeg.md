---
layout: post
title:  "ffmpeg笔记"
date:   2017-03-20 15:15:54
categories: ffmpeg
excerpt: ffmpeg linux note
---

* content
{:toc}


近一两年内开源项目中对我工作影响和帮助最大的非ffmpeg莫属，ffmpeg的强大以及被使用的普遍性令人惊讶。每次阅读ffmpeg的源代码都感觉收获颇多，经常将从ffmpeg里面吸收的代码技巧和音视频算法用于公司的实际项目中，解决实际问题的同时也提示自己的代码质量。感谢开源的精神与力量。


---

## ffmpeg中基本对象

<br />

理解ffmpeg中主要基本对象的实现细节对于分析ffmpeg中编解码流程至关重要，尤其是编解码中内存的管理。我们知道c语言实现的项目，相当一部分工作就是内存的管理，内存的管理是难点，也是重点。ffmpeg中关于内存管理的技巧值得我们好好研究学习。

<br />

### AVBuffer

<br />

AVBuffer是ffmpeg中内存管理的基石，是其他对象(AVPacket、AVFrame)的元对象，其代码实现非常的有技术含量。

AVBuffer对象分成AVBuffer和AVBufferRef两个不同层次的对象，其中AVBuffer为私有对象(*it is opaque and not meant to be accessed by the caller directly*),它表示的是内存数据的本身，实现定义在buffer_internal.h头文件中。

	struct AVBuffer {
	    uint8_t *data; /**< data described by this buffer */
	    int      size; /**< size of data in bytes */

	    /**
	     *  number of existing AVBufferRef instances referring to this buffer
	     */
	    atomic_uint refcount;

	    /**
	     * a callback for freeing the data
	     */
	    void (*free)(void *opaque, uint8_t *data);

	    /**
	     * an opaque pointer, to be used by the freeing callback
	     */
	    void *opaque;

	    /**
	     * A combination of BUFFER_FLAG_*
	     */
	    int flags;
	};

AVBufferRef表示的是对AVBuffer描述的内存数据的引用，它是一个对外的接口对象，定义在buffer.h头文件中。

	/**
	 * A reference to a data buffer.
	 *
	 * The size of this struct is not a part of the public ABI and it is not meant
	 * to be allocated directly.
	 */
	typedef struct AVBufferRef {
	    AVBuffer *buffer;

	    /**
	     * The data buffer. It is considered writable if and only if
	     * this is the only reference to the buffer, in which case
	     * av_buffer_is_writable() returns 1.
	     */
	    uint8_t *data;
	    /**
	     * Size of data in bytes.
	     */
	    int      size;
	} AVBufferRef;

为什么要在AVBuffer对象的基础上再设计出一个AVBufferRef对象呢？从软件设计模式(dedign pattern)来说，这是一种代码模式([proxy](https://en.wikipedia.org/wiki/Proxy_pattern))实现。如果提供API接口直接访问AVBuffer对象，基于引用计数实现免拷贝的机制不太好实现，另外，对于
多线程下读写buffer内存数据的控制也不太好实现。其实，最大的好处是，可以很方便的定义API函数来操作被代理的对象，而不是直接操作对象变量。

AVBufferRef对象中的data字段主要是为了写访问avBuffer的控制，这个字段也是最容易引起混淆和不解的地方。而其实，AVBuffer对象绝大部分情况下都是以只读
方式访问的，因为引用计数已经实现为原子操作，所以referencing 和 unreferencing avBuffer是多线程安全的，无须额外加锁保护。

AVBuffer组件不只是提供了一般的buffer创建与读写访问接口，还提供了内存池的功能，以实现某些业务下需要内存池技术解决频繁申请释放内存带来的副作用。

avBuffer池主要由AVBufferPool对象和BufferPoolEntry对象实现，这两个对象都是对外不可见的opaque对象，也定义在buffer_internal.h中。

	typedef struct BufferPoolEntry {
	    uint8_t *data;

	    /*
	     * Backups of the original opaque/free of the AVBuffer corresponding to
	     * data. They will be used to free the buffer when the pool is freed.
	     */
	    void *opaque;
	    void (*free)(void *opaque, uint8_t *data);

	    AVBufferPool *pool;
	    struct BufferPoolEntry *next;
	} BufferPoolEntry;


	struct AVBufferPool {
	    AVMutex mutex;
	    BufferPoolEntry *pool;

	    /*
	     * This is used to track when the pool is to be freed.
	     * The pointer to the pool itself held by the caller is considered to
	     * be one reference. Each buffer requested by the caller increases refcount
	     * by one, returning the buffer to the pool decreases it by one.
	     * refcount reaches zero when the buffer has been uninited AND all the
	     * buffers have been released, then it's safe to free the pool and all
	     * the buffers in it.
	     */
	    atomic_uint refcount;

	    int size;
	    void *opaque;
	    AVBufferRef* (*alloc)(int size);
	    AVBufferRef* (*alloc2)(void *opaque, int size);
	    void         (*pool_free)(void *opaque);
	};	
	
通过上面的对象结构体定义，以及avbuffer池的几个API接口可见，avbuffer池功能的实现很简单，就是在avbuffer基本功能的基础上添加了回收复用使用结束的avbuffer对象。

<br />

**avBuffer的创建**

为了兼容所管理的内存来自于外部独立申请的情况(*例如，GPU内存*)，avBuffer提供了av_buffer_alloc()、av_buffer_create()两个API用于创建avBuffer对象。

	/**
	 * Allocate an AVBuffer of the given size using av_malloc().
	 *
	 * @return an AVBufferRef of given size or NULL when out of memory
	 */
	AVBufferRef *av_buffer_alloc(int size);
	

	/**
	 * Create an AVBuffer from an existing array.
	 *
	 * If this function is successful, data is owned by the AVBuffer. The caller may
	 * only access data through the returned AVBufferRef and references derived from
	 * it.
	 * If this function fails, data is left untouched.
	 * @param data   data array
	 * @param size   size of data in bytes
	 * @param free   a callback for freeing this buffer's data
	 * @param opaque parameter to be got for processing or passed to free
	 * @param flags  a combination of AV_BUFFER_FLAG_*
	 *
	 * @return an AVBufferRef referring to data on success, NULL on failure.
	 */
	AVBufferRef *av_buffer_create(uint8_t *data, int size,
				      void (*free)(void *opaque, uint8_t *data),
				      void *opaque, int flags);
	
	
从函数原型入参来看，av_buffer_alloc()接口创建的内存对象是由avBuffer组件内部调用系统默认内存申请函数创建的，而av_buffer_create()接口需要
调用者提供已经申请好的内存指针data,以及如何释放这块内存的方法。

如果我们使用的是avbuffer池的功能，那么在调用av_buffer_pool_init()或者av_buffer_pool_init2()后，通过调用av_buffer_pool_get()接口创建新的
avbuffer对象或者是复用之前创建并且已经被释放回avbuffer池的avbuffer对象。

实现av_buffer_pool_init()的两个版本也是为了兼容所管理的内存来自于外部独立申请的情况，

	/**
	 * Allocate and initialize a buffer pool.
	 *
	 * @param size size of each buffer in this pool
	 * @param alloc a function that will be used to allocate new buffers when the
	 * pool is empty. May be NULL, then the default allocator will be used
	 * (av_buffer_alloc()).
	 * @return newly created buffer pool on success, NULL on error.
	 */
	AVBufferPool *av_buffer_pool_init(int size, AVBufferRef* (*alloc)(int size));
	
	
	/**
	 * Allocate and initialize a buffer pool with a more complex allocator.
	 *
	 * @param size size of each buffer in this pool
	 * @param opaque arbitrary user data used by the allocator
	 * @param alloc a function that will be used to allocate new buffers when the
	 *              pool is empty.
	 * @param pool_free a function that will be called immediately before the pool
	 *                  is freed. I.e. after av_buffer_pool_uninit() is called
	 *                  by the caller and all the frames are returned to the pool
	 *                  and freed. It is intended to uninitialize the user opaque
	 *                  data.
	 * @return newly created buffer pool on success, NULL on error.
	 */
	AVBufferPool *av_buffer_pool_init2(int size, void *opaque,
					   AVBufferRef* (*alloc)(void *opaque, int size),
					   void (*pool_free)(void *opaque));

要注意的是，如果使用avbuffer池接口来管理外部申请的内存，那么提供给av_buffer_pool_init2()中的alloc回调函数，其实现中必须使用av_buffer_create()
接口来创建新avbuffer对象。

<br />

**引用与释放引用**

通过引用的API函数实现av_buffer_ref()可以看出，使用高一层的对象代理实际的对象的好处，对实际对象的操作可以很好的打包封装起来，不至于散落在各个角落。

	AVBufferRef *av_buffer_ref(AVBufferRef *buf)
	{
	    AVBufferRef *ret = av_mallocz(sizeof(*ret));

	    if (!ret)
		return NULL;

	    *ret = *buf;

	    atomic_fetch_add_explicit(&buf->buffer->refcount, 1, memory_order_relaxed);

	    return ret;
	}

因为使用了两层对象，在释放引用时，要记住也需要两层释放对象，代理层的avBufferRef对象在非avBuffer池接口和池接口流程中释放引用时始终都需要被free掉。

无论是否使用avBuffer池，引用和释放引用的调用的接口和流程一模一样，因为avBuffer对象中的free方法在两者创建时别注册了不同的实现。

<br />

**写访问**

ffmpeg中avBuffer对象大部分情况下用于只读访问。写访问相关的接口主要是av_buffer_realloc()函数和av_buffer_make_writable()函数。前者实现很简单，后者
比较复杂，容易混淆。

	int av_buffer_make_writable(AVBufferRef **pbuf)
	{
	    AVBufferRef *newbuf, *buf = *pbuf;

	    if (av_buffer_is_writable(buf))
		return 0;

	    newbuf = av_buffer_alloc(buf->size);
	    if (!newbuf)
		return AVERROR(ENOMEM);

	    memcpy(newbuf->data, buf->data, buf->size);

	    buffer_replace(pbuf, &newbuf);

	    return 0;
	}

上面实现中需要注意和理解的是代理对象中data字段的使用，很容易和实际对象avBuffer中的data混淆。

> @note Two different references to the same buffer can point to different
> parts of the buffer (i.e. their AVBufferRef.data will not be equal).

结合上面这段头文件中的注释，好好体会代理对象在写访问中的作用。


<br />

---

## ffmpeg硬解(vaapi-decode)流程分析

<br />

本章节主要关注的是ffmpeg中硬件解码h264的流程分析，而其中的解码算法细节实现暂不展开。已故雷神雷宵铧的博客中关于ffmpeg的h264解码已经比较详细的分析，
但是不同版本的ffmpeg解码部分函数实现差别很大，其博客中的一些函数分析已经过时，而且没有涉及硬件解码的情况。但是雷神文章中对于ffmpeg解码的总体流程分析还是很有指导意义的，对雷神
的钻研精神表示敬意。

本文都是基于较新的v3.3.3版本的ffmpeg进行代码分析。

<br />

### 主体流程

<br />

ffmpeg中使用AVCodec结构体描述解码器对象，其中h264解码器为实现在src/libavcodec/h264dec.c中的ff_h264_decoder实例。无论是软解还是硬解，解码流程的入口都是avcodec_decodec_video2()这个API接口调用ff_h264_decoder中注册的h264_decode_frame()方法。

	/*libavcodec/h264dec.c*/
	static int h264_decode_frame(AVCodecContext *avctx, void *data, int *get_frame, AVPacket *avpkt)
	{
		...
		
		/**
		 * end of stream, output what is still in the buffers
		 */
		if(buf_size == 0)
			return send_next_delayed_frame(h, pict, got_frame, 0);
			
		...
		
		decode_nal_units(h, buf, buf_size);//[0]
		 
		...
		
		ff_h264_field_end(h, &h->slice_ctx[0], 0);//[1]
		
		/**
		 * wait for second field.
		 */
	
		if(h->next_output_pic)
			finalize_frame(h, pict, h->next_output_pic, got_frame);//[2]
			
		...	
	}

上面的主要处理流程是，[0]处进行码流分析，如果是软解就接着进行解码工作，如果是硬件就只设置解码器的参数，然后在[1]处进行实际的硬解码，[2]处是控制
POC，决定输出哪一帧。

	/*libavcodec/h264dec.c*/
	static int decodec_nal_units(H264Context *h, const uint8_t *buf, int buf_size)
	{
		...
		
		/**
		 * 拆分协议信息。
		 */
		ff_h245_packet_split(&h->pkt, buf, buf_size, avctx, ...);
		
		...
		
		/**
		 * 对上面拆分出的各个NALU进行逐个解析。
		 */
		for(i = 0; i < h->pkt.nb_nals; i++)
		{
			...
			
			switch(nal->type){
				case H264_NAL_IDR_SLICE:
				case H264_NAL_SLICE_SCALABLE:
				case H264_NAL_SLICE:
					h->has_slice = 1;
					
					ff_h264_queue_decode_slice(h, nal);//[0]
					
					if(h->current_slice == 1){
						...
						
						/**
						 * avctx中的hwaccel句柄如果有效，说明开启的是vaapi流程。
						 */
						if(h->avctx->hwaccel)
							h->avctx->hwaccel->start_frame(...);	
							
						...	
					}
					
					max_slice_ctx = avctx->hwaccel ? 1 : h->nb_silce_ctx;
					if(h->nb_slice_ctx_queued == max_slice_ctx){
						if(h->avctx->hwaccel){
							/**
							 * 这里只是进行硬件解码器的参数设置。
							 */
							avctx->hwaccel->decoded_slice(...);
							h->nb_slice_ctx_queued = 0;
						}else{
							...
							/**
							 * 如果是软解，调用
							 * ff_h264_execute_decode_slice()进行解码。
							 */
						}
					}
					
					break;
					
				case H264_NAL_SEI:
					/**
					 * 解析SEI信息。
					 */
					 ff_h264_sei_decode(&h->sei, &nal->gb, &h->ps, avctx);
					
				case H264_NAL_SPS:
					/**
					 * 解析sps信息。
					 */
					ff_h264_decode_seq_parameter_set(&tmp_gb, avctx, &h->ps, 0);
					...
				break;
				
				case H264_NAL_PPS:
					/**
					 * 解析PPS信息。
					 */
					ff_h264_decode_picture_parameter_set(...);
					...
				break;
				
				case xxx:
					...
				
			}
			
			/**
			 * error handle
			 */
			 ...
		}
	}

上面处理中，主要分成两个阶段，一个是解析非slice类型的NALU，包括PPS,SPS,SEI等，另外一个就是解析slice。而解析非slice类型NALU也是
为了解析slice服务的。上面[0]处就是解析slice，实现比较复杂，涉及到帧解码、场解码的兼容；根据不同POC类型计算POC值等等。

当slice信息也解析完成后，最终解析码流的所需要的所有信息已经拿到了，下面就是开启硬件解码器(*hwaccel->start_frame()*)，并将解析的信息按需传入
(*hwaccel->decoded_slice()*)。

下面先分析[0]处的ff_h264_queue_decode_slice()函数实现。

	int ff_h264_queue_decode_slice(H264Context *h, H2645NAL *nal)
	{
		H264SliceContext *sl = h->slice_ctx + h->nb_slice_ctx_queued;
		int first_slice = sl = h->slice_ctx && !h->current_slice;
		
		...
		
		if(nal->type == H264_NAL_SLICE_SCALABLE && nal->svc_ext_flag == 1)
			h264_slice_header_in_scalable_ext_parse(h, sl, nal);//[0]
		else
			h264_slice_header_parse(h, sl, nal);//[1]
			
		...
		
		if(sl->first_mb_addr == 0){
			if(h->current_slice){
				/**
				 * 场解码处理
				 */
				...
			}
			/**
			 * 场解码处理
			 */
			...
		}
		
		if(!first_slice){
			/**
			 * 场解码中pps和sps一致性检测。
			 */
			...
		}
		
		if(h->current_slice == 0){
			/**
			 * 帧解码处理、或者是场解码中上半场处理。
			 */
			h264_field_start(h, sl, nal, first_slice);//[3]
		
		}else{
			/**
			 * 场解码中一致性检测。
			 */
			...
		}
		
		h264_slice_init(h, sl, nal);//[4]
	
		h->nb_slice_ctx_queued++;
		
		return 0;
	}

上面代码中[0],[1]功能类似，都是解析slice的头信息，解析时结合起前面解析出的pps和sps信息，计算出frame_num；如果poc_type是0的话，需要计算出poc_lsb，以及qp相关的信息。

上面很多逻辑都是针对于场解码的，这部分细节暂不展开。从[3]处的代码可以看出，h264_field_start()兼容处理了帧解码和场解码情况，因为可以将帧解码看成是场解码的一种特殊情况，通过这样的抽象可以提高代码的复用程度，这时软件实现中常用的技巧。下面就来看看h264_field_start()函数的具体实现:

	/**
	 * This function is called right after decoding the slice header for a first
	 * slice in a field(or a frame). It decides whether we are decoding a new frame
	 * or a second field in a pair and does the necessary setup
	 */
	static int h264_field_start(...)
	{
		...
		
		/**
		 * shorten frame num gaps so we don't have to allocate refrence 
		 * frames just to throw them away.
		 */
		if(h->poc.frame_num != h->poc.prev_frame_num){
			...
		}
		
		/**
		 * see if we have a decoded first field looking for a pair...
		 * Here, we're using that to see if we should mark previously decode frame
		 * as "finished". W have to do that before the "dummy" in-between frame allocation,
		 * since that can modify h->cur_pic_ptr.
		 */
		if(h->first_filed){
			...
		}
		
		...
		
		/**
		 * see if we have a decoded first field looking for a pair...
		 * we're using that to see whether to continue decoding in that
		 * frame, or to allocate a new one.
		 */
		if(h->first_field){
		
		}else{
			/*frame or first field in a potentially complementary pair*/
			h->first_field = FIELD_PICTURE(h);
		}
		
		if(!FIELD_PICTURE(h) || h->first_field){
			h264_frame_start(h);//[0]
		}else{
			...
		}
		
		...
		
		ff_h264_init_poc(...);//[1]
		
		...
		
		/**
		 * set the frame properties/side data. Only done for the second field in 
		 * field coded frames, since some SEI information is present for each field
		 * and is merged by the SEI parsing code.
		 */
		if(!FIELD_PICTURE(h) || !h->first_field || h->missing_fileds > 1){
			...
			
			h264_select_output_frame(h);//[2]
		}
		
		return 0;
	}
	

上面代码中仍然很多逻辑控制都是针对于场解码的，而[0]处是只针对于帧解码的，[1][2]处是帧解码和场解码通用的处理。[1]处从函数名称就可以看出，是计算poc的，具体实现就是根据poc_type的类型来区分计算，细节可以参考h264的ITU。上面代码功能的重点是[0]处申请解码输出内存，以及[2]处对于解码后帧是否输出、以及顺序的控制。下面分别分析这两个函数的实现。

	static int h264_frame_start(H264Context *h)
	{
		...
		
		release_unused_pictures(h, 1);//[0]
		h->cur_pic_ptr = NULL;
		
		i = find_unused_picture(h);
		
		pic = &h->DPB[i];
		
		...
		
		/**
		 * Zero key_frame here; IDR markings per slice in frame or fields are ORed in later.
		 */
		pic->f->key_frame = 0; 
		...
		
		alloc_picture(h, pic);//[1]
		
		h->cur_pic_ptr = pic;
		ff_h264_unref_picture(h, &h->cur_pic);
		
		...
		
		ff_h264_ref_picture(h, &h->cur_pic, h->cur_pic_ptr);
		
		...
		
		/**
		 * we mark the current picture as non-reference after allocating it, so
		 * that if we break out due to an error it can be released automatically 
		 * in the next ff_mpv_frmae_start().
		 */
		h->cur_pic_ptr->reference = 0;
		
		h->next_output_pic = NULL;
	
		...
		
		return 0;
	}
	
上面代码主要是通过[1]处申请解码输出的内存，对于硬件解码，输出内存为GPU的内存。申请出的内存会作为参考帧由DPB管理，DPB是在解码器初始化时申请的，
关于解码器的初始化，后面有单独小节详细分析。下面先来看看alloc_picture()的实现：

	static int alloc_picture(H264Context *h, H264Picture *pic)
	{
		...
		
		pic->tf.f = pic->f;
		/**
		 * 在硬件解码中，该函数会调用我们在初始化时注册的申请GPU内存的
		 * 的回调函数avctx->get_buffer2.
		 */
		ff_thread_get_buffer(h->avctx, &pic->tf, pic->reference ? AV_GET_BUFFER_FLAGS_REF:0);//[0]
		
		...
		
		/**
		 * 分配硬件解码vaapi对象，该对象在vaapi_h264.c文件中注册为ff_h264_vaapi_hwaccel。
		 * 这个vaapi对象作为h264解码器对象的priv域，相对于一个后端引擎。
		 * 这是软件分层设计的常规手法。
		 */
		if(h->avctx->hwaccel){
			const AVHWAccel *hwaccel = h->avctx->hwaccel;
			if(hwaccel->frame_priv_data_size){
				pic->hwaccel_priv_buf = av_buffer_allocz(hwaccel->frame_priv_data_size);
				
				pic->hwaccel_picture_private = pic->hwaccel_priv_buf->data;
			}
		}
		
		...
		
		return ret;
	}

上面代码主要是申请GPU内存，以及创建vaapi解码对象。[0]处最终会调用get_buffer_internal()函数，其实现如下：

	static int get_buffer_internal(AVCodecContext *avctx, AVFrame *frame, int flags)
	{
		const AVHWAccel *hwaccel = avctx->hwaccel;
		
		...
		
		if(hwaccel){
			if(hwaccel->alloc_frame){
				hwaccel->alloc_frame(avctx, frame);//[0]
			}
		}else{
			avctx->sw_pix_fmt = avctx->pix_fmt;
		}
		
		avctx->get_buffer2(avctx, frame, flag);//[1]
			
		...
		
		return ret;
	}
	
因为上面代码中[0]处在vaapi_h264对象中并没有定义，所以这个函数的功能主要就是[1]处调用avctx->get_buffer2方法。依据GPU的不同，get_buffer2
的回调实现各异，但是总体流程一致，需要完成的流程如下:

	static int va_get_buffer(struct AVCodecContext *avctx, AVFrame *pic, int flags)
	{
		AVBufferRef *surf_buf = NULL;
		vaapi_surface_t *surface = NULL;
		
		...
		
		surface = xxxx;//[0]
		
		surf_buf = av_buffer_create(NULL, 0, free_surf_buf, surface, AV_BUFFER_FLAG_READONLY);
		
		pic->buf[0] = surf_buf;
		/**
		 * 软解时使用data[0-2]传递解码数据的CPU内存地址，
		 * 硬解时使用data[3]传递解码后GPU内存句柄，即surface_id。
		 */
		pic->data[3] = surface;
	
		return 0;
	}

上面代码[0]处是需要根据不同GPU自己实现的申请GPU内存的方法，然后将描述这块内存的指针和释放内存的方法传入av_buffer_create()接口函数。
最后，重点是将pic->buf和pic->data正确赋值。

到这里，h264_frame_start()函数的所有实现细节基本剖析完成了，下面回过头来分析一下h264_select_output_frame()函数的实现。

h264_select_output_frame()函数是纯逻辑控制，通过之前计算出的poc来调整显示输出帧的顺序，下面只贴出函数中决定解码完成后将要
输出的帧的部分代码:

	static int h264_select_output_frame(H264Context *h)
	{
		H264Picture *cur = h->cur_pic_ptr;
	
		...
	
		pics = 0;
		while(h->delayed_pic[pics])
			pics++;
	
		h->delayed_pic[pics++] = cur;
	
		...
	
		if(!out_of_order || pics > h->avctx->has_b_frames){
		        /**
			 * next_output_pic就是指向了下个要输出的帧，这帧可能是
			 * 之前解码后缓存到h->delayed_pic[]中的，也可能就是上面通过调用
			 * va_get_buffer()刚刚创建的GPU用来存储解码后数据的内存。
			 */
			h->next_output_pic = out;
			
			...
		}
		
		...
	}
	
到这里，ff_h264_queue_decode_slice()函数实现就剖析完成了，之后，对于软解来说直接调用ff_h264_execute_decode_slices()完成
最终实际的解码工作，而对于硬解，需要先传递解码器需要的参数信息来配置解码器，也就是上面分析decode_nal_units()函数调用的
avctx->hwaccel->start_frame()和avctx->hwaccel->decode_slice()两个vaapi_h264对象提供的方法。


关于VAAPI的调用流程，见[这篇笔记](http://saiyn.github.io/homepage/2017/03/02/gpu/#vaapi调用流程)。

vaapi_h264对象中start_frame()和decode_slice()方法对应完成的工作，就是上面这篇笔记中描述的基本流程中的第三点，即填充各种decode buffers。
也就是调用vaapi接口vaCreateBuffer()创建各种buffer，前者创建的buffer类型包含VAPictureParameterBufferType、VAQMatrixBufferType；后者创建的为VASliceParameterBufferType、VASliceDataBUfferType。

到了这里，对于硬件解码来说，解码的前期准备工作算是完成了，下面就是真正开始解码工作，也就是硬件解码引擎真正运作。这部分代码定义在ff_h264_filed_end()函数中，也是h264_decode_frame()接口调用的倒数第二个函数。

	int ff_h264_field_end(H264Context *h. H264SliceContext *sl, int in_setup)
	{
		AVCodecContext *const avctx = h->avctx;
		
		...
		
		if(avctx->hwaccel){
			avctx->hwaccel->end_frame(avctx);
		}
	
		...
	}

可以看出来，ff_h264_field_end()函数完全是为了硬件解码而设计的，因为软解在之前都已经直接完成了。
vaapi_h264对象中的end_frame()方法完成的是上面这篇笔记中描述的基本流程中的最后一点，即将上面的buffers传输到后端server，让
后端server完成最终的解码。最后一个流程的API调用比较模式化，都是：

`vaBeginPicture()` -> `vaRenderPicture()` -> `vaEndPicture()`

解码最后一步就是输出解码后的帧，这个功能finalize_frame()函数完成。

	static int finalize_frame(H264Context *h, AVFrame *dst, H264Picture *out, int *got_frame)
	{
		...
		
		output_frame(h, dst, out);
		
		*got_frame = 1;
	
		...
	}

	static int output_frame(H264Context *h, AVFrame *dst, H264Picture *srcp)
	{
		AVFrame *src = srcp->f;
		
		...
		
		av_frame_ref(dst, src);
		
		...
	}

dst是我们在调用avcodec_decode_video2()API时传入的输出帧，av_frame_ref(dst,src)完成了解码帧的输出。
至此，硬件加速解码的主体流程分析完毕，下面回过头来分析一下解码前的初始化流程。下面以函数调用图来总结一下上面的流程分析。

![ffmpeg_1](http://omp8s6jms.bkt.clouddn.com/image/git/ffmpeg_1.png)

	
<br />

### 帧内存管理流程

<br />

下面先通过一个流程图，总体上说明一下解码时，解码数据的控制与流向。

![ffmpeg_0](http://omp8s6jms.bkt.clouddn.com/image/git/ffmpeg_0.png)

<br />

---

## ffmpeg硬编(vaapi-encode)流程分析

<br />

---

## 解决ffmpeg解码首帧I帧不输出问题

<br />




<br />

---

## ffserver

<br />

ffserver is a multimedia streaming server for live broadcasts. With it, you can streamover HTTP,RTP and RSTP.

![ffserver](https://www.alkannoide.com/wp-content/uploads/2013/07/ffserver_map.png)

<br />

### 配置文件

<br />

研究ffserver得从配置文件开始，doc/ffserver.conf 是配置文件的模板。ffserver　reads a configuration file containing global options and settings for each stream and feed.

主要的语法规则:
The configuration file consists of global options and dedicated sections, which must be introduced by "<SECTION_NAME ARGS>" on a separate line and must be terminated by a line in the form "</SECTION_NAME>". ARGS is a optional.

下面分别说明配置文件中的各个模块

<br />

**Feed section**

Feed 就是输入源的意思，这个配置模块就是告诉ffserver关于输入源的一些信息。比如这个输入源的名字，这个源数据量的大小，以及哪些地址的客户端可以接受这个源数据。You must use 'ffmpeg' to send a live feed to ffserver. In this example, you can type:ffmpeg http://localhost:8090/feed1.ffm. 第一种方式就是使用ffmpeg程序执行命令产生实时的数据流，比如执行`ffmpeg -i INPUTFILE http://localhost:8090/feed1.ffm`,其中`feed1.ffm`就是这个源的名字。ffserver can also do time shifting.It means that it can stream any previously recorded live stream.You must specify a path where the feed is stored on disk.You alsa specify the maximum size of the feed,where 0 means unlimited.第二种就是从硬盘中加载已经录制好的源，当然格式必须是.ffm的。这样我们可以写个feed配置块。

![feed1](http://omp8s6jms.bkt.clouddn.com/image/git/feed1.png)

<br />

**Stream section**

在说明stream配置模块前，先说明一下feed和stream的关系。A "live-stream" or "sream" is a resource published by ffserver,and made accessible through the HTTP protocol to cliens.	A stream can be connected to a feed, or to a file. In the first case, the published stream is forwarded from the corresponding feed generated by a running instance of ffpmeg.也就是说，stream是对feed的封装。Multiple streams can be connected to the same feed.下面是一个实例图:

![graph](http://omp8s6jms.bkt.clouddn.com/image/git/graph.png)

<br />

stream配置中主要包含三块内容，这个stream是对接的哪个feed；stream的格式;stream中的音视频码流参数。下面是ffserver目前支持的格式:

mpeg	|MPEG-1 multiplexed video and audio
mpegvideo	|only MPEG-1 video
mp2	|MPEG-2 audio(use AudioCodec to select layer 2 and 3 codec)
ogg	|ogg format(vorbis audio codec)
rm	|RealNetworks-compatible stream.Multiplexed audio and video.
ra	|RealNetworks-compatible stream.Audio only.
mpjpeg	|Multipart JPEG 
jpeg	|Generate a single JPEG image
mjpeg	|Generate a M_JPEG stream
asf	|ASF compatible streaming(Windows Media Player format)
swf	|Macromedia Flash compatible stream
avi	|AVI format(MPEG-4 video, MPEG audio sound)

<br />

下面是一个配置实例:

![stream2](http://omp8s6jms.bkt.clouddn.com/image/git/stream2.png)

<br />

最后，还要一个special stream.用来在网页上监控ffserver的状态信息。

![stream3](http://omp8s6jms.bkt.clouddn.com/image/git/stat.png)

<br />

This page will help us to moniter the server.This page is call with this url:http://localhost:8090/stat.html and you will get this page.执行如下的命令:

![ffser_cmd](http://omp8s6jms.bkt.clouddn.com/image/git/ffse_ffp.png)

<br />

得到如下的状态：

![status](http://omp8s6jms.bkt.clouddn.com/image/git/ffserver.png)

<br />

从上图可以看到我们已经成功的创建了一个名为`test1.mpg`的流，在另外一个终端上执行`ffplay http://localhost:8090/test1.mpg`或者执行`cvlc http://localhost:8090/test1.mpg`都可以进行拉流播放。



---


















