---
layout: post
title:  "ffmpeg笔记"
date:   2017-03-20 15:15:54
categories: ffmpeg
excerpt: ffmpeg linux note
---

* content
{:toc}


近一两年内开源项目中对我工作影响和帮助最大的非ffmpeg莫属，ffmpeg的强大以及被使用的普遍性令人惊讶。每次阅读ffmpeg的源代码都感觉收获颇多，经常将从ffmpeg里面吸收的代码技巧和音视频算法用于公司的实际项目中，解决实际问题的同时也提示自己的代码质量。感谢开源的精神与力量。


---

## ffmpeg硬解(vaapi-decode)流程分析

<br />

本章节主要关注的是ffmpeg中硬件解码h264的流程分析，而其中的解码算法细节实现暂不展开。已故雷神雷宵铧的博客中关于ffmpeg的h264解码已经比较详细的分析，
但是不同版本的ffmpeg解码部分函数实现差别很大，其博客中的一些函数分析已经过时，而且没有涉及硬件解码的情况。但是雷神文章中对于ffmpeg解码的总体流程分析还是很有指导意义的，对雷神
的钻研精神表示敬意。

本文都是基于较新的v3.3.3版本的ffmpeg进行代码分析。

ffmpeg中使用AVCodec结构体描述解码器对象，其中h264解码器为实现在src/libavcodec/h264dec.c中的ff_h264_decoder实例。无论是软解还是硬解，解码流程的入口都是avcodec_decodec_video2()这个API接口调用ff_h264_decoder中注册的h264_decode_frame()方法。

	/*libavcodec/h264dec.c*/
	static int h264_decode_frame(AVCodecContext *avctx, void *data, int *get_frame, AVPacket *avpkt)
	{
		...
		
		/**
		 * end of stream, output what is still in the buffers
		 */
		if(buf_size == 0)
			return send_next_delayed_frame(h, pict, got_frame, 0);
			
		...
		
		decode_nal_units(h, buf, buf_size);//[0]
		 
		...
		
		ff_h264_field_end(h, &h->slice_ctx[0], 0);//[1]
		
		/**
		 * wait for second field.
		 */
	
		if(h->next_output_pic)
			finalize_frame(h, pict, h->next_output_pic, got_frame);//[2]
			
		...	
	}

上面的主要处理流程是，[0]处进行码流分析，如果是软解就接着进行解码工作，如果是硬件就只设置解码器的参数，然后在[1]处进行实际的硬解码，[2]处是控制
POC，决定输出哪一帧。

	/*libavcodec/h264dec.c*/
	static int decodec_nal_units(H264Context *h, const uint8_t *buf, int buf_size)
	{
		...
		
		/**
		 * 拆分协议信息。
		 */
		ff_h245_packet_split(&h->pkt, buf, buf_size, avctx, ...);
		
		...
		
		/**
		 * 对上面拆分出的各个NALU进行逐个解析。
		 */
		for(i = 0; i < h->pkt.nb_nals; i++)
		{
			...
			
			switch(nal->type){
				case H264_NAL_IDR_SLICE:
				case H264_NAL_SLICE_SCALABLE:
				case H264_NAL_SLICE:
					h->has_slice = 1;
					
					ff_h264_queue_decode_slice(h, nal);//[0]
					
					if(h->current_slice == 1){
						...
						
						/**
						 * avctx中的hwaccel句柄如果有效，说明开启的是vaapi流程。
						 */
						if(h->avctx->hwaccel)
							h->avctx->hwaccel->start_frame(...);	
							
						...	
					}
					
					max_slice_ctx = avctx->hwaccel ? 1 : h->nb_silce_ctx;
					if(h->nb_slice_ctx_queued == max_slice_ctx){
						if(h->avctx->hwaccel){
							/**
							 * 这里只是进行硬件解码器的参数设置。
							 */
							avctx->hwaccel->decoded_slice(...);
							h->nb_slice_ctx_queued = 0;
						}else{
							...
							/**
							 * 如果是软解，调用
							 * ff_h264_execute_decode_slice()进行解码。
							 */
						}
					}
					
					break;
					
				case H264_NAL_SEI:
					/**
					 * 解析SEI信息。
					 */
					 ff_h264_sei_decode(&h->sei, &nal->gb, &h->ps, avctx);
					
				case H264_NAL_SPS:
					/**
					 * 解析sps信息。
					 */
					ff_h264_decode_seq_parameter_set(&tmp_gb, avctx, &h->ps, 0);
					...
				break;
				
				case H264_NAL_PPS:
					/**
					 * 解析PPS信息。
					 */
					ff_h264_decode_picture_parameter_set(...);
					...
				break;
				
				case xxx:
					...
				
			}
			
			/**
			 * error handle
			 */
			 ...
		}
	}

上面处理中，主要分成两个阶段，一个是解析非slice类型的NALU，包括PPS,SPS,SEI等，另外一个就是解析slice。而解析非slice类型NALU也是
为了解析slice服务的。上面[0]处就是解析slice，实现比较复杂，涉及到帧解码、场解码的兼容；根据不同POC类型计算POC值等等。

当slice信息也解析完成后，最终解析码流的所需要的所有信息已经拿到了，下面就是开启硬件解码器(*hwaccel->start_frame()*)，并将解析的信息按需传入
(*hwaccel->decoded_slice()*)。

下面先分析[0]处的ff_h264_queue_decode_slice()函数实现。

	int ff_h264_queue_decode_slice(H264Context *h, H2645NAL *nal)
	{
		H264SliceContext *sl = h->slice_ctx + h->nb_slice_ctx_queued;
		int first_slice = sl = h->slice_ctx && !h->current_slice;
		
		...
		
		if(nal->type == H264_NAL_SLICE_SCALABLE && nal->svc_ext_flag == 1)
			h264_slice_header_in_scalable_ext_parse(h, sl, nal);//[0]
		else
			h264_slice_header_parse(h, sl, nal);//[1]
			
		...
		
		if(sl->first_mb_addr == 0){
			if(h->current_slice){
				/**
				 * 场解码处理
				 */
				...
			}
			/**
			 * 场解码处理
			 */
			...
		}
		
		if(!first_slice){
			/**
			 * 场解码中pps和sps一致性检测。
			 */
			...
		}
		
		if(h->current_slice == 0){
			/**
			 * 帧解码处理、或者是场解码中上半场处理。
			 */
			h264_field_start(h, sl, nal, first_slice);//[3]
		
		}else{
			/**
			 * 场解码中一致性检测。
			 */
			...
		}
		
		h264_slice_init(h, sl, nal);//[4]
	
		h->nb_slice_ctx_queued++;
		
		return 0;
	}

上面代码中[0],[1]功能类似，都是解析slice的头信息，解析时结合起前面解析出的pps和sps信息，计算出frame_num；如果poc_type是0的话，需要计算出poc_lsb，以及qp相关的信息。

上面很多逻辑都是针对于场解码的，这部分细节暂不展开。从[3]处的代码可以看出，h264_field_start()兼容处理了帧解码和场解码情况，因为可以将帧解码看成是场解码的一种特殊情况，通过这样的抽象可以提高代码的复用程度，这时软件实现中常用的技巧。下面就来看看h264_field_start()函数的具体实现:

	/**
	 * This function is called right after decoding the slice header for a first
	 * slice in a field(or a frame). It decides whether we are decoding a new frame
	 * or a second field in a pair and does the necessary setup
	 */
	static int h264_field_start(...)
	{
		...
		
		/**
		 * shorten frame num gaps so we don't have to allocate refrence 
		 * frames just to throw them away.
		 */
		if(h->poc.frame_num != h->poc.prev_frame_num){
			...
		}
		
		/**
		 * see if we have a decoded first field looking for a pair...
		 * Here, we're using that to see if we should mark previously decode frame
		 * as "finished". W have to do that before the "dummy" in-between frame allocation,
		 * since that can modify h->cur_pic_ptr.
		 */
		if(h->first_filed){
			...
		}
		
		...
		
		/**
		 * see if we have a decoded first field looking for a pair...
		 * we're using that to see whether to continue decoding in that
		 * frame, or to allocate a new one.
		 */
		if(h->first_field){
		
		}else{
			/*frame or first field in a potentially complementary pair*/
			h->first_field = FIELD_PICTURE(h);
		}
		
		if(!FIELD_PICTURE(h) || h->first_field){
			h264_frame_start(h);//[0]
		}else{
			...
		}
		
		...
		
		ff_h264_init_poc(...);//[1]
		
		...
		
		/**
		 * set the frame properties/side data. Only done for the second field in 
		 * field coded frames, since some SEI information is present for each field
		 * and is merged by the SEI parsing code.
		 */
		if(!FIELD_PICTURE(h) || !h->first_field || h->missing_fileds > 1){
			...
			
			h264_select_output_frame(h);//[2]
		}
		
		return 0;
	}
	





---

## ffmpeg硬编(vaapi-encode)流程分析

<br />

---

## 解决ffmpeg解码首帧I帧不输出问题

<br />




<br />

---

## ffserver

<br />

ffserver is a multimedia streaming server for live broadcasts. With it, you can streamover HTTP,RTP and RSTP.

![ffserver](https://www.alkannoide.com/wp-content/uploads/2013/07/ffserver_map.png)

<br />

### 配置文件

<br />

研究ffserver得从配置文件开始，doc/ffserver.conf 是配置文件的模板。ffserver　reads a configuration file containing global options and settings for each stream and feed.

主要的语法规则:
The configuration file consists of global options and dedicated sections, which must be introduced by "<SECTION_NAME ARGS>" on a separate line and must be terminated by a line in the form "</SECTION_NAME>". ARGS is a optional.

下面分别说明配置文件中的各个模块

<br />

**Feed section**

Feed 就是输入源的意思，这个配置模块就是告诉ffserver关于输入源的一些信息。比如这个输入源的名字，这个源数据量的大小，以及哪些地址的客户端可以接受这个源数据。You must use 'ffmpeg' to send a live feed to ffserver. In this example, you can type:ffmpeg http://localhost:8090/feed1.ffm. 第一种方式就是使用ffmpeg程序执行命令产生实时的数据流，比如执行`ffmpeg -i INPUTFILE http://localhost:8090/feed1.ffm`,其中`feed1.ffm`就是这个源的名字。ffserver can also do time shifting.It means that it can stream any previously recorded live stream.You must specify a path where the feed is stored on disk.You alsa specify the maximum size of the feed,where 0 means unlimited.第二种就是从硬盘中加载已经录制好的源，当然格式必须是.ffm的。这样我们可以写个feed配置块。

![feed1](http://omp8s6jms.bkt.clouddn.com/image/git/feed1.png)

<br />

**Stream section**

在说明stream配置模块前，先说明一下feed和stream的关系。A "live-stream" or "sream" is a resource published by ffserver,and made accessible through the HTTP protocol to cliens.	A stream can be connected to a feed, or to a file. In the first case, the published stream is forwarded from the corresponding feed generated by a running instance of ffpmeg.也就是说，stream是对feed的封装。Multiple streams can be connected to the same feed.下面是一个实例图:

![graph](http://omp8s6jms.bkt.clouddn.com/image/git/graph.png)

<br />

stream配置中主要包含三块内容，这个stream是对接的哪个feed；stream的格式;stream中的音视频码流参数。下面是ffserver目前支持的格式:

mpeg	|MPEG-1 multiplexed video and audio
mpegvideo	|only MPEG-1 video
mp2	|MPEG-2 audio(use AudioCodec to select layer 2 and 3 codec)
ogg	|ogg format(vorbis audio codec)
rm	|RealNetworks-compatible stream.Multiplexed audio and video.
ra	|RealNetworks-compatible stream.Audio only.
mpjpeg	|Multipart JPEG 
jpeg	|Generate a single JPEG image
mjpeg	|Generate a M_JPEG stream
asf	|ASF compatible streaming(Windows Media Player format)
swf	|Macromedia Flash compatible stream
avi	|AVI format(MPEG-4 video, MPEG audio sound)

<br />

下面是一个配置实例:

![stream2](http://omp8s6jms.bkt.clouddn.com/image/git/stream2.png)

<br />

最后，还要一个special stream.用来在网页上监控ffserver的状态信息。

![stream3](http://omp8s6jms.bkt.clouddn.com/image/git/stat.png)

<br />

This page will help us to moniter the server.This page is call with this url:http://localhost:8090/stat.html and you will get this page.执行如下的命令:

![ffser_cmd](http://omp8s6jms.bkt.clouddn.com/image/git/ffse_ffp.png)

<br />

得到如下的状态：

![status](http://omp8s6jms.bkt.clouddn.com/image/git/ffserver.png)

<br />

从上图可以看到我们已经成功的创建了一个名为`test1.mpg`的流，在另外一个终端上执行`ffplay http://localhost:8090/test1.mpg`或者执行`cvlc http://localhost:8090/test1.mpg`都可以进行拉流播放。



---


















