---
layout: post
title:  "Machine Learning笔记"
date:   2019-06-21 15:15:54
categories: ML
excerpt: ML machine learning 
---

* content
{:toc}


# Basic

<br />

## SVM(Support Vector Machine)

<br />

### concept

SVM是线性分类器中的一种，它的基本思想是: Maximize the margin size
与普通的线性分类器只是确保能将训练数据分正确不同的是，SVM在正确的情况下力求最大化边界距离，这样可以在目标数据杂讯较多时也能保证正确分类。
本质上就是提高分类器的容错能力

### 调参(基于scikit-learn)

    from sklearn.svm import SVC
    
    model = SVC(C=1.0,
                kernel = 'linear',
                max_iter = 1000,
                tol = 0.0001)
    model.fit(X,y)


* C - C是cost function的“惩罚系数”， C值越小，容忍度越高，意思就是在训练数据，较多的容忍一些越界数据。

* class_weight - 

* tol and max_iter - 最大收敛次数限制

* kernel - 控制线性可分还是非线性可分


    from sklearn.svm import LinearSVC
    
    svm = LinearSVC(C= 1.0, class_weight=None, fit_intercept=1,
                    intercept_scaling=1, loss='squared_hinge', max_iter=1000,
                    multi_class='ovr', penalty='l2', random_state=0, tol=1e-04)
                    
                    


* multi_class - 绝大数情况下，我们是要处理多类别的分类，对应的分类器有两种工作模式:OVR(one over all)、OVO(one over one).前者在分类时，总是在区分
                某个类别时把剩余的当做一个整体作为另外一类， 所以只需要建k个分类器；而后者需要每两个类都建立分类器，所以总共需要建立k(k -1)/2个分类器。
                
* 
                
                





