---
layout: post
title:  "tun/tap"
date:   2022-02-15 15:15:54
categories: Linux-Network
excerpt: network tun iptables systemd
---

* content
{:toc}

---

# 基本概念

tun/tap为用户空间的程序提供了收发数据包的能力，不同于物理网口从内核层接受ip数据包，tun/tap从用户层接受，同理，tun/tap向用户程序发送数据而不是向内核发送。

tun/tap本质是一个虚拟设备，一端是内核网络接口，另一端是用户空间的文件描述符。它有两种工作模式：

* Tap mode, 工作于L2数据链路层，主要用于vm
* Tun mode, 工作于L3网络层，主要用于vpn一类的应用


> TUN/TAP provides packet reception and transmission for user space programs. It can be > seen as a simple Point-to-Point or Ethernet device

上面是内核文档中描述tun/tap的一段，其中提到了tun设备其实是一种`Point-to-Point`网络设备，那怎么理解呢？

POINT2POINT表示该网络接口上不存在L2的选址功能：

* 没有ARP请求(ipv4)
* 没有NDP请求(ipv6)
* 不涉及内核层的邻居子系统(neighbour layer)
* 不支持路由表中`via`指令
* 该接口上的数据包只能送到the same(only) next-hop

和tap或者其他正常的网口(Ethernet device)对比来说，tun网口的一端只会连接一个设备，而Ethernet设备往往都连接很多设备，需要通过neighbour layer去处理不同的连接。


# 网络模型

![tun_tap_0](https://raw.githubusercontent.com/saiyn/homepage/gh-pages/images/tun_tap_0.png)


tun设备在应用层暴露的是fd，提供应用读写数据的接口，在内核层和物理网卡一样对接的是网络栈，另外从图中看出很重要的一点就是，tun设备在内核层是没有`ring buffer`的，这就导致如果tun设备的`TX queue`满了之后就会丢失数据包。


下面通过一个具体的测试，来更加直观的了解网络数据是如何在tun设备中流动的。


假设我们已经通过`/dev/net/tun`内核文件对象创建了`tun0`, 然后执行:

```
    ip addr add 192.168.1.1/24 dev tun0

    ip route add 10.1.1.0/24 via 192.168.1.2

    ping 10.1.1.2 -I 10.1.1.1

```

![tun_tap_1](https://raw.githubusercontent.com/saiyn/homepage/gh-pages/images/tun_tap_0.png)


上面的app A是ping应用程序，app B可以是openvpn进程。


数据包的发送流程为:

* ping应用程序将数据包通过系统调用发送到内核网络协议栈
* 协议栈根据数据包的目的IP地址，匹配本地路由规则，知道这个数据包



# 内核代码实现






# 应用实例

## tun2sockes


## tun2http






